#Download data
! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d johnsmith88/heart-disease-dataset

! unzip heart-disease-dataset.zip -d /content

#Import packages
import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import datetime
import os
import keras
from tensorflow.keras import layers

#1. Read csv
file_path = r"/content/heart.csv"
heart_data = pd.read_csv(file_path)

#2. Data cleaning
#check for NaN
print(heart_data.isna().sum())

#plots to observe datatype
heart_data.hist()

#5. One hot encode label
heart_data= pd.get_dummies(heart_data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal','target'])

#Check the one-hot label
print("--------------------One-hot Label-----------------")
print(heart_data.head())

print(heart_data.head())

#3. Split to features and label
label_name = ['target_0','target_1']
heart_features = heart_data.copy()
heart_label = pd.concat([heart_data.pop(x) for x in label_name],axis=1)

#4. Check Data
print("------------------Features-------------------------")
print(heart_features.head())
print("-----------------Label----------------------")
print(heart_label.head())

#6. Split the features and labels into train-validation-test sets

SEED = 12345
x_train, x_iter, y_train, y_iter = train_test_split(heart_features,heart_label,test_size=0.4,random_state=SEED)
x_val, x_test, y_val, y_test = train_test_split(x_iter,y_iter,test_size=0.5,random_state=SEED)

#7. Normalize
scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_val = scaler.transform(x_val)
x_test = scaler.transform(x_test)

#data preparation completed

#Build model
#8. Build NN model
nIn = x_train.shape[-1]
nClass = y_train.shape[-1]

#Use functional API
inputs = keras.Input(shape=(nIn,))
h1 = layers.Dense(64,activation='relu')
h2 = layers.Dense(32,activation='relu')
h3 = layers.Dense(16,activation='relu')
out = layers.Dense(nClass,activation='softmax')

x = h1(inputs)
x = h2(x)
x = h3(x)
outputs = out(x)

model = keras.Model(inputs=inputs,outputs=outputs)
model.summary()

# Commented out IPython magic to ensure Python compatibility.
#9. Compile and train the model
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping
#load tensorboard
# %load_ext tensorboard
# Prepare the early stopping and tensorboard callbacks
base_log_path = r"/content/results"
log_path = os.path.join(base_log_path, datetime.datetime.now().strftime("%Y%m%d-%H%M%S")+'test2')
#es = EarlyStopping(monitor='val_loss',patience=10)

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_path, histogram_freq=1)
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
history = model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size=32,epochs=20,callbacks=[tensorboard_callback])

from keras.utils.vis_utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/results
